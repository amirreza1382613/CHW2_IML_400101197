{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChC3RF8meAlK"
      },
      "source": [
        "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
        "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
        "\n",
        "\n",
        "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
        "\n",
        "\n",
        "**Student Name**:Amirreza Dehghani\n",
        "\n",
        "**Student ID**:400101197\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IraiR0SbeDi_"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRQjwWC3eDnc"
      },
      "source": [
        "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import numpy as np\n",
        "import cupy as cp  # For GPU support\n",
        "\n",
        "class MyLogisticRegression:\n",
        "    # Your code goes here!\n",
        "    # This class must have an __init__ method, a loss function, a fit function, and a predict function. You also need to make your code runnable on gpu!\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def initialize_parameters(self, n_features):\n",
        "        self.weights = np.zeros((n_features, 1))\n",
        "        self.bias = 0\n",
        "\n",
        "    def compute_cost(self, A, Y):\n",
        "        m = Y.shape[0]\n",
        "        cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m\n",
        "        return cost\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        m, n = X.shape\n",
        "        self.initialize_parameters(n)\n",
        "\n",
        "        for i in range(self.num_iterations):\n",
        "            Z = np.dot(X, self.weights) + self.bias\n",
        "            A = self.sigmoid(Z)\n",
        "            m = len(Y)  # Number of instances\n",
        "            Y =  np.asarray(Y).reshape((m, 1))\n",
        "            dZ = A - Y\n",
        "            dW = np.dot(X.T, dZ) / m\n",
        "            db = np.sum(dZ) / m\n",
        "\n",
        "            self.weights -= self.learning_rate * dW\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                cost = self.compute_cost(A, Y)\n",
        "                print(f\"Iteration {i}: Cost = {cost}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        Z = np.dot(X, self.weights) + self.bias\n",
        "        A = self.sigmoid(Z)\n",
        "        predictions = (A > 0.5).astype(int)\n",
        "        return predictions\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-i-oubUlZ6e"
      },
      "source": [
        "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KXzIy_2u-pG",
        "outputId": "9625f7e2-abb1-4591-c0fa-843525e0ffd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: Cost = 0.6931471805599453\n",
            "Iteration 100: Cost = 0.1910609667784817\n",
            "Iteration 200: Cost = 0.18957002286134758\n",
            "Iteration 300: Cost = 0.18868596530275444\n",
            "Iteration 400: Cost = 0.18792420409024385\n",
            "Iteration 500: Cost = 0.18724275304898672\n",
            "Iteration 600: Cost = 0.18662929944413031\n",
            "Iteration 700: Cost = 0.1860749061600116\n",
            "Iteration 800: Cost = 0.18557202371934245\n",
            "Iteration 900: Cost = 0.18511417905291894\n",
            "Accuracy: 0.875\n",
            "Precision: 0.875\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9333333333333333\n",
            "Confusion Matrix:\n",
            " [[ 0 10]\n",
            " [ 0 70]]\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "X, Y = None, None\n",
        "\n",
        "### START CODE HERE ###\n",
        "data_logistic=pd.read_csv('Logistic_question.csv')\n",
        "X=data_logistic.iloc[:, :7]\n",
        "Y=data_logistic['Target']\n",
        "### END CODE HERE ###\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "Y_train = (Y_train > 0.5).astype(int)\n",
        "lg = MyLogisticRegression()\n",
        "lg.fit(X = X_train, Y = Y_train)\n",
        "Y_val_pred = lg.predict(X_temp)\n",
        "Y_temp = (Y_temp > 0.5).astype(int)\n",
        "accuracy = accuracy_score(Y_temp, Y_val_pred)\n",
        "precision = precision_score(Y_temp, Y_val_pred)\n",
        "recall = recall_score(Y_temp, Y_val_pred)\n",
        "f1 = f1_score(Y_temp, Y_val_pred)\n",
        "conf_matrix = confusion_matrix(Y_temp, Y_val_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji0RXNGKv1pa"
      },
      "source": [
        "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldveD35twRRZ"
      },
      "source": [
        "**Your answer:**\n",
        "In logistic regression, commonly used evaluation metrics include:\n",
        "\n",
        "1.Accuracy: Accuracy measures the proportion of correctly classified instances out of the total instances. It is calculated as the ratio of the number of correct predictions to the total number of predictions. \n",
        "\n",
        "   - Accuracy is a straightforward metric that provides an overall view of the model's performance.\n",
        "   - It is suitable for balanced datasets where the classes are relatively evenly distributed.\n",
        "   - However, accuracy may not be an appropriate metric for imbalanced datasets, where one class is much more prevalent than the others.\n",
        "\n",
        "2.Precision: Precision measures the proportion of true positive predictions out of all positive predictions (both true positives and false positives). It focuses on the accuracy of positive predictions.\n",
        "\n",
        "   - Precision is useful when the cost of false positives is high, and we want to minimize the number of false positives.\n",
        "   - For example, in a medical diagnosis task, precision indicates the proportion of correctly identified positive cases out of all predicted positive cases.\n",
        "\n",
        "3.Recall (Sensitivity): Recall measures the proportion of true positive predictions out of all actual positive instances. It focuses on the model's ability to correctly identify positive instances.\n",
        "\n",
        "   - Recall is important when the cost of false negatives is high, and we want to minimize the number of false negatives.\n",
        "   - For example, in a spam email detection task, recall indicates the proportion of correctly identified spam emails out of all actual spam emails.\n",
        "\n",
        "4.F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is particularly useful when the class distribution is imbalanced.\n",
        "\n",
        "   - The F1 score is suitable for tasks where both false positives and false negatives are equally important.\n",
        "   - It is a single metric that combines information from both precision and recall, making it useful for comparing models and selecting the best performing one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZCeRHZSw-mh"
      },
      "source": [
        "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vb5lRSQXDLR3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9375\n",
            "Precision: 0.9333333333333333\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9655172413793104\n",
            "Confusion Matrix:\n",
            " [[ 5  5]\n",
            " [ 0 70]]\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "# Your code goes here!\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Instantiate the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "Y_train = (Y_train > 0.5).astype(int)\n",
        "Y_temp = (Y_temp > 0.5).astype(int)\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "Y_val_pred = model.predict(X_temp)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(Y_temp, Y_val_pred)\n",
        "precision = precision_score(Y_temp, Y_val_pred)\n",
        "recall = recall_score(Y_temp, Y_val_pred)\n",
        "f1 = f1_score(Y_temp, Y_val_pred)\n",
        "conf_matrix = confusion_matrix(Y_temp, Y_val_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCvIymmMy_ji"
      },
      "source": [
        "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0ohM16z3De"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClMqoYlr2kr7"
      },
      "source": [
        "# Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukvlqDe52xP5"
      },
      "source": [
        "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5Ir-_hFt286t"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MyMultinomialLogisticRegression:\n",
        "    def __init__(self, num_classes, num_features):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = num_features\n",
        "        self.weights = np.zeros((num_features, num_classes))\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "\n",
        "    def compute_cost(self, X, y, reg_lambda):\n",
        "        m = X.shape[0]\n",
        "        Z = np.dot(X, self.weights)\n",
        "        probabilities = self.softmax(Z)\n",
        "        loss = -np.sum(np.log(probabilities[range(m), y])) / m\n",
        "        reg_term = 0.5 * reg_lambda * np.sum(self.weights ** 2)\n",
        "        cost = loss + reg_term\n",
        "        return cost\n",
        "\n",
        "    def compute_gradient(self, X, y, reg_lambda):\n",
        "        m = X.shape[0]\n",
        "        Z = np.dot(X, self.weights)\n",
        "        probabilities = self.softmax(Z)\n",
        "        error = probabilities\n",
        "        error[range(m), y] -= 1\n",
        "        gradient = np.dot(X.T, error) / m + reg_lambda * self.weights\n",
        "        return gradient\n",
        "\n",
        "    def fit(self, X, y, reg_lambda=0.01, learning_rate=0.01, num_iterations=100):\n",
        "        for i in range(num_iterations):\n",
        "            gradient = self.compute_gradient(X, y, reg_lambda)\n",
        "            self.weights -= learning_rate * gradient\n",
        "\n",
        "    def predict(self, X):\n",
        "        Z = np.dot(X, self.weights)\n",
        "        probabilities = self.softmax(Z)\n",
        "        return np.argmax(probabilities, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPQ3Rtay3Y2_"
      },
      "source": [
        "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9aP4QJPq29B3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.3625\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "\n",
        "model = MyMultinomialLogisticRegression(10, 7)\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "Y_train = Y_train.values\n",
        "Y_temp = Y_temp.values\n",
        "for i in range(len(Y_train)):\n",
        "    Y_train[i] = (int(Y_train[i] * 100)//10)\n",
        "for i in range(len(Y_temp)):\n",
        "    Y_temp[i] = (int(Y_temp[i] * 100)//10)\n",
        "\n",
        "# Train the model on the training data\n",
        "Y_train = Y_train.astype(int)\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "Y_val_pred = model.predict(X_temp)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(Y_temp, Y_val_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy, for classes = 9: 0.3625\n",
            "Accuracy, for classes = 8: 0.475\n",
            "Accuracy, for classes = 7: 0.675\n",
            "Accuracy, for classes = 6: 0.7625\n",
            "Accuracy, for classes = 5: 0.875\n",
            "Accuracy, for classes = 4: 0.85\n",
            "Accuracy, for classes = 3: 0.8375\n",
            "Accuracy, for classes = 2: 0.85\n"
          ]
        }
      ],
      "source": [
        "model = MyMultinomialLogisticRegression(10, 7)\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "Y_train = Y_train.values\n",
        "Y_temp = Y_temp.values\n",
        "\n",
        "for j in range(1, 9):\n",
        "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "    Y_train = Y_train.values\n",
        "    Y_temp = Y_temp.values\n",
        "    for i in range(len(Y_train)):\n",
        "        Y_train[i] = (int(Y_train[i] * 100)//(j * 10))\n",
        "    for i in range(len(Y_temp)):\n",
        "        Y_temp[i] = (int(Y_temp[i] * 100)//(j * 10))\n",
        "    Y_train = Y_train.astype(int)\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    Y_val_pred = model.predict(X_temp)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy = accuracy_score(Y_temp, Y_val_pred)\n",
        "\n",
        "    # Print the evaluation metrics\n",
        "    print(f\"Accuracy, for classes = {10 - j}:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of2sHl5Z4dXi"
      },
      "source": [
        "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRLERDAr4wnS"
      },
      "source": [
        "**Your answer:**as you can see for i=5 we have the best perform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT43jGKV6CBZ"
      },
      "source": [
        "# Going a little further!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo9uGo0R6GZo"
      },
      "source": [
        "First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "o-vrjYBF7u1E",
        "outputId": "b274bc6e-4c35-4ad8-f17b-9e69f7d92923"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Use this to select the kaggle.json file from your computer\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i6u6_1v8ftX"
      },
      "source": [
        "Then use this code to automatically download the dataset into Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjyVaVKF29Hx",
        "outputId": "15d0b1a2-c806-4102-abbc-12545237e218"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d wenruliu/adult-income-dataset\n",
        "!unzip /content/adult-income-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=pd.read_csv('adult.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQnbZwt8rJK"
      },
      "source": [
        "**Task:** Determine the number of null entries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtuEx6QW29c1",
        "outputId": "43397bec-0622-4dc4-de2b-c65be00e4503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of null entries: 6465\n",
            "Number of null entries in each column:\n",
            "age                   0\n",
            "workclass          2799\n",
            "fnlwgt                0\n",
            "education             0\n",
            "educational-num       0\n",
            "marital-status        0\n",
            "occupation         2809\n",
            "relationship          0\n",
            "race                  0\n",
            "gender                0\n",
            "capital-gain          0\n",
            "capital-loss          0\n",
            "hours-per-week        0\n",
            "native-country      857\n",
            "income                0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "null_counts = (data == \"?\").sum()\n",
        "\n",
        "# Sum all the null counts to get the total number of null entries\n",
        "total_null_entries = null_counts.sum()\n",
        "\n",
        "print(\"Total number of null entries:\", total_null_entries)\n",
        "print(\"Number of null entries in each column:\")\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpEcBdTUAYVN"
      },
      "source": [
        "**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1u1pBHuAsSg"
      },
      "source": [
        "**Your answer:**\n",
        "\n",
        "1.Deletion: This involves removing rows or columns with null values. You can either remove the entire row if it contains null values (row-wise deletion) or remove the entire column if it contains too many null values (column-wise deletion). This method is straightforward but may lead to loss of valuable data.\n",
        "\n",
        "2.Imputation: Imputation involves replacing null values with substitute values. Some common imputation techniques include replacing null values with the mean, median, mode, or a \n",
        "constant value. This method preserves the dataset's size but may introduce bias if the imputed values are not representative of the actual data.\n",
        "\n",
        "3.Forward Fill or Backward Fill: In time-series data, where null values represent missing observations at specific time points, you can use forward fill (filling with the last observed value) or backward fill (filling with the next observed value) to impute null values. This method maintains the temporal order of data but may not be suitable for non-time-series data.\n",
        "\n",
        "4.Predictive Modeling: Use machine learning algorithms to predict null values based on the non-null values and other features in the dataset. For example, you can train a regression model to predict numerical null values or a classification model to predict categorical null values. This method can be computationally intensive but may provide accurate imputations.\n",
        "\n",
        "5.Multiple Imputation: This method involves creating multiple imputed datasets by replacing null values with different imputed values in each iteration. These datasets are then analyzed separately, and the results are combined using specific rules (e.g., averaging). Multiple imputation accounts for uncertainty in imputed values and can provide more robust results compared to single imputation methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHhH-hkpAxFf"
      },
      "source": [
        "**Task:** Handle null entries using your best method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5fVwWcjK29fk",
        "outputId": "c21a6adf-1e6c-46d0-dd61-79d1710272c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of null entries after imputation:\n",
            "age                0\n",
            "workclass          0\n",
            "fnlwgt             0\n",
            "education          0\n",
            "educational-num    0\n",
            "marital-status     0\n",
            "occupation         0\n",
            "relationship       0\n",
            "race               0\n",
            "gender             0\n",
            "capital-gain       0\n",
            "capital-loss       0\n",
            "hours-per-week     0\n",
            "native-country     0\n",
            "income             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "data[numerical_columns] = data[numerical_columns].fillna(data[numerical_columns].mean())\n",
        "\n",
        "# Impute null values with mode for categorical columns\n",
        "categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
        "\n",
        "# Verify if there are any null values left\n",
        "null_counts_after_imputation = data.isnull().sum()\n",
        "print(\"Number of null entries after imputation:\")\n",
        "print(null_counts_after_imputation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43k5cTorCJaV"
      },
      "source": [
        "**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agj18Lcd-vyZ",
        "outputId": "69e132a9-0249-4a21-c8f3-45247c1e17dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:993: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 982, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 415, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 993, in score\n",
            "    Xt = transform.transform(Xt)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1014, in transform\n",
            "    Xs = self._call_func_on_transformers(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 823, in _call_func_on_transformers\n",
            "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1283, in _transform_one\n",
            "    res = transformer.transform(X, **params.transform)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1023, in transform\n",
            "    X_int, X_mask = self._transform(\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 213, in _transform\n",
            "    raise ValueError(msg)\n",
            "ValueError: Found unknown categories ['Holand-Netherlands'] in column 7 during transform\n",
            "\n",
            "  warnings.warn(\n",
            "c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:993: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 982, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 415, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 993, in score\n",
            "    Xt = transform.transform(Xt)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1014, in transform\n",
            "    Xs = self._call_func_on_transformers(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 823, in _call_func_on_transformers\n",
            "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1283, in _transform_one\n",
            "    res = transformer.transform(X, **params.transform)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1023, in transform\n",
            "    X_int, X_mask = self._transform(\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 213, in _transform\n",
            "    raise ValueError(msg)\n",
            "ValueError: Found unknown categories ['Holand-Netherlands'] in column 7 during transform\n",
            "\n",
            "  warnings.warn(\n",
            "c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:993: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 982, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 415, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 993, in score\n",
            "    Xt = transform.transform(Xt)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1014, in transform\n",
            "    Xs = self._call_func_on_transformers(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 823, in _call_func_on_transformers\n",
            "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1283, in _transform_one\n",
            "    res = transformer.transform(X, **params.transform)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1023, in transform\n",
            "    X_int, X_mask = self._transform(\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 213, in _transform\n",
            "    raise ValueError(msg)\n",
            "ValueError: Found unknown categories ['Holand-Netherlands'] in column 7 during transform\n",
            "\n",
            "  warnings.warn(\n",
            "c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:993: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 982, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 415, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 993, in score\n",
            "    Xt = transform.transform(Xt)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1014, in transform\n",
            "    Xs = self._call_func_on_transformers(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 823, in _call_func_on_transformers\n",
            "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1283, in _transform_one\n",
            "    res = transformer.transform(X, **params.transform)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1023, in transform\n",
            "    X_int, X_mask = self._transform(\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 213, in _transform\n",
            "    raise ValueError(msg)\n",
            "ValueError: Found unknown categories ['Holand-Netherlands'] in column 7 during transform\n",
            "\n",
            "  warnings.warn(\n",
            "c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:993: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 982, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 415, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 993, in score\n",
            "    Xt = transform.transform(Xt)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1014, in transform\n",
            "    Xs = self._call_func_on_transformers(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 823, in _call_func_on_transformers\n",
            "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1283, in _transform_one\n",
            "    res = transformer.transform(X, **params.transform)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1023, in transform\n",
            "    X_int, X_mask = self._transform(\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 213, in _transform\n",
            "    raise ValueError(msg)\n",
            "ValueError: Found unknown categories ['Holand-Netherlands'] in column 7 during transform\n",
            "\n",
            "  warnings.warn(\n",
            "c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:993: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 982, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 415, in __call__\n",
            "    return estimator.score(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 993, in score\n",
            "    Xt = transform.transform(Xt)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1014, in transform\n",
            "    Xs = self._call_func_on_transformers(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 823, in _call_func_on_transformers\n",
            "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1283, in _transform_one\n",
            "    res = transformer.transform(X, **params.transform)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1023, in transform\n",
            "    X_int, X_mask = self._transform(\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 213, in _transform\n",
            "    raise ValueError(msg)\n",
            "ValueError: Found unknown categories ['Holand-Netherlands'] in column 7 during transform\n",
            "\n",
            "  warnings.warn(\n",
            "c:\\Users\\shargh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2'}\n",
            "Train Accuracy: 0.8366135182862846\n",
            "Test Accuracy: 0.8410277408127751\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop(columns=['income'])  \n",
        "y = data['income']\n",
        "\n",
        "# Split the dataset into training and testing sets with an 80-20 portion\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing steps for numerical and categorical features\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "categorical_transformer = OneHotEncoder()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Define the pipeline with preprocessing and Logistic Regression\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression(solver='lbfgs'))])  # Use 'lbfgs' solver\n",
        "\n",
        "# Define parameters for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'classifier__penalty': ['l2']  # Use only 'l2' penalty\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to find the best parameters\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Report the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on the training set and calculate accuracy\n",
        "y_train_pred = grid_search.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "\n",
        "# Predict on the test set and calculate accuracy\n",
        "y_test_pred = grid_search.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lzr2lqXDQ1T"
      },
      "source": [
        "**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9D1jlstF9nF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QS9HYJ5FW1T"
      },
      "source": [
        "**Question:** Explain your proposed methods and the reason you decided to use them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCBQuAeF46a"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjSREvg4FTHf"
      },
      "source": [
        "**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfKS-Jq0-v4P"
      },
      "outputs": [],
      "source": [
        "# Your code goes here!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWV0YUgRGg1p"
      },
      "source": [
        "**Question:** Analyze the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer:**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
